{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yOWsoAVf_tLI",
        "omNoUAC-_nBI",
        "V5G4C7SE_cun",
        "KlRxYqsO_UxP",
        "c7pnKfN3_LvH",
        "Axkq0Nb6-4ae",
        "JGCK297U-08v",
        "XkJM6Wg9-wPo",
        "RfxIjeVP-pF_",
        "Fy5F4cJB-lWS",
        "Zjz9arXJ-gDG",
        "GcBBzbo5-c54",
        "caKbfgxq-Qce",
        "_MJ6JcQo-Vpe"
      ],
      "authorship_tag": "ABX9TyNmbHkT/mkqSv19l0Ndf/Vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaneRLos/Conv-LSTM-SAR-XAI/blob/main/IEEE_2024_Conv_LSTM_XAI_SAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bLzYDFfl_r-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Datasets"
      ],
      "metadata": {
        "id": "yOWsoAVf_tLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CooWa8GF5wbP"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "\n",
        "# Load altimetry dataset from Google Drive\n",
        "normalized_altimetry_dataset = xr.open_dataset(\"/content/drive/MyDrive/Altimetry_Normalized/Normalized_Altimetry_Merged_V3.nc\")\n",
        "\n",
        "# Print first few lines :)\n",
        "print(normalized_altimetry_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the normalized merged buoy dataset from Google Drive\n",
        "merged_df = pd.read_csv('/content/drive/MyDrive/buoy-M3_year_long/merged_normalized_buoy_data_V3.csv')\n",
        "\n",
        "# Corrected approach without using 'inplace=True'\n",
        "merged_df['WaveHeight'] = merged_df['WaveHeight'].interpolate(method='linear')\n",
        "\n",
        "# Remove rows where 'WindSpeed' is NaN directly within the same DataFrame\n",
        "merged_df.dropna(subset=['WindSpeed'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the merged dataset and its shape\n",
        "merged_df_head = merged_df.head()\n",
        "merged_df_shape = merged_df.shape\n",
        "\n",
        "merged_df_head, merged_df_shape"
      ],
      "metadata": {
        "id": "ZhQ1rkf26iUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAR Image Preprocessing"
      ],
      "metadata": {
        "id": "omNoUAC-_nBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILTER, NORMAL, RESIZE, STACK AND LOAD\n",
        "\n",
        "import rasterio\n",
        "from scipy.ndimage import uniform_filter\n",
        "import numpy as np\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import cv2\n",
        "\n",
        "def lee_filter(img, size):\n",
        "    img_mean = uniform_filter(img, size)\n",
        "    img_sqr_mean = uniform_filter(img**2, size)\n",
        "    img_variance = img_sqr_mean - img_mean**2\n",
        "    overall_variance = np.var(img)\n",
        "    img_weights = img_variance / (img_variance + overall_variance)\n",
        "    return img_mean + img_weights * (img - img_mean)\n",
        "\n",
        "def read_and_preprocess_sar_image(file_path, target_size=(128, 128)):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        sar_image = src.read(1, masked=True)  # Read the first band as a masked array\n",
        "\n",
        "        # Apply the Lee filter\n",
        "        filtered_image = lee_filter(sar_image.data, size=3)  # Size of the filter window\n",
        "\n",
        "        # Mask out the no-data values again after filtering\n",
        "        filtered_image = np.ma.array(filtered_image, mask=sar_image.mask, fill_value=np.nan)\n",
        "\n",
        "        # Check for NaN values after filtering\n",
        "        if np.isnan(filtered_image).any():\n",
        "            print(f\"NaN values found in the filtered image: {file_path}\")\n",
        "            return None  # Skip saving this image\n",
        "\n",
        "        # Normalize the filtered image\n",
        "        min_val = filtered_image.min()\n",
        "        max_val = filtered_image.max()\n",
        "        normalized_image = (filtered_image - min_val) / (max_val - min_val)\n",
        "\n",
        "        # Resize the image using OpenCV\n",
        "        resized_image = cv2.resize(normalized_image.filled(), target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        return resized_image\n",
        "\n",
        "def save_processed_image(file_path, processed_image, output_folder):\n",
        "    output_filename = os.path.basename(file_path).replace('.tif', '_processed_resized.tif')\n",
        "    output_path = os.path.join(output_folder, output_filename)\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    with rasterio.open(file_path) as src:\n",
        "        profile = src.profile\n",
        "        profile.update(dtype=rasterio.float32, height=processed_image.shape[0], width=processed_image.shape[1], count=1, compress='lzw')\n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(processed_image.astype(rasterio.float32), 1)\n",
        "\n",
        "def process_file(file_path, output_folder):\n",
        "    if os.path.exists(file_path):\n",
        "        processed_image = read_and_preprocess_sar_image(file_path)\n",
        "        if processed_image is not None:  # Only save if there are no NaN values\n",
        "            save_processed_image(file_path, processed_image, output_folder)\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Set input and output folders\n",
        "input_folder = '/content/drive/MyDrive/ content drive MyDrive Final'\n",
        "output_folder = '/content/drive/MyDrive/Processed_Images_10k_V1'\n",
        "\n",
        "# Get all .tif files that are neither processed nor interpolated\n",
        "sar_files = [\n",
        "    os.path.join(input_folder, f)\n",
        "    for f in os.listdir(input_folder)\n",
        "    if f.endswith('.tif') and not (f.endswith('_processed.tif') or f.endswith('_interpolated.tif'))\n",
        "]\n",
        "\n",
        "# Process each SAR file in parallel\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    executor.map(process_file, sar_files, [output_folder] * len(sar_files))\n",
        "\n",
        "# LOAD PROCESSED SAR IMAGES INTO NUMPY ARRAYS\n",
        "\n",
        "def load_image_rasterio(image_path):\n",
        "    try:\n",
        "        with rasterio.open(image_path) as src:\n",
        "            # Read the first band\n",
        "            img = src.read(1)\n",
        "            img = np.expand_dims(img, axis=-1)  # Add channel dimension for grayscale\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# List all processed and resized files\n",
        "image_paths = [os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith('_processed_resized.tif')]\n",
        "\n",
        "# Test loading one of the images\n",
        "test_img_path = image_paths[0] if image_paths else None\n",
        "test_img = load_image_rasterio(test_img_path) if test_img_path else None\n",
        "print(\"Is the image loaded?\", test_img is not None)\n",
        "\n",
        "# If the test image is successful, load all images\n",
        "if test_img is not None:\n",
        "    # Load all resized images if they're not None, and convert them to NumPy array\n",
        "    sar_files = [load_image_rasterio(path) for path in image_paths]\n",
        "    sar_files = np.stack([img for img in sar_files if img is not None])\n",
        "\n",
        "    print(\"Number of SAR images:\", len(sar_files))\n"
      ],
      "metadata": {
        "id": "-2K4c2Wm6wuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timestamps"
      ],
      "metadata": {
        "id": "V5G4C7SE_cun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Altimetry Timestamps"
      ],
      "metadata": {
        "id": "KlRxYqsO_UxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify start and end dates for the range\n",
        "start_date = '2015-05-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "# Generate the date range at monthly frequency\n",
        "date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "\n",
        "# Convert the date range to a list of tuples (start_date, end_date) for each month\n",
        "date_ranges = [(date.strftime('%Y-%m-%dT00:00:00Z'), (date + pd.offsets.MonthEnd(1)).strftime('%Y-%m-%dT00:00:00Z')) for date in date_range]\n",
        "\n",
        "# For debugging: print the first few date ranges\n",
        "for start, end in date_ranges[:5]:\n",
        "    print(f\"('{start}', '{end}')\")\n"
      ],
      "metadata": {
        "id": "05jxzhp17G6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the function to select altimetry data for a given timestamp\n",
        "def select_altimetry_data_for_timestamp(altimetry_dataset, timestamp, tolerance=0):\n",
        "    start_date = timestamp - pd.Timedelta(seconds=tolerance)\n",
        "    end_date = timestamp + pd.Timedelta(seconds=tolerance)\n",
        "\n",
        "    for time in altimetry_dataset['time']:\n",
        "        if start_date <= time <= end_date:\n",
        "            # Select altimetry data within the specified date range\n",
        "            time_selection = altimetry_dataset.sel(time=time)\n",
        "            return time_selection\n",
        "\n",
        "    return None\n",
        "\n",
        "# Print some information about the altimetry dataset for inspection\n",
        "print(\"Altitude Dataset Information:\")\n",
        "print(normalized_altimetry_dataset)"
      ],
      "metadata": {
        "id": "XWzvZI1p7LJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buoy Data Timestamps"
      ],
      "metadata": {
        "id": "c7pnKfN3_LvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buoy Data Filtering\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the 'time' column in the buoy data to datetime format\n",
        "merged_df['time'] = pd.to_datetime(merged_df['time'])\n",
        "\n",
        "# Prepare an empty DataFrame to store filtered buoy data\n",
        "filtered_buoy_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each date range and filter the data\n",
        "for start_date, end_date in date_ranges:\n",
        "    # Convert start and end times to datetime\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date = pd.to_datetime(end_date)\n",
        "    temp_filtered_data = merged_df[(merged_df['time'] >= start_date) & (merged_df['time'] <= end_date)]\n",
        "    filtered_buoy_data = pd.concat([filtered_buoy_data, temp_filtered_data])\n",
        "\n",
        "# Reset the index of the concatenated DataFrame\n",
        "filtered_buoy_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the first few rows of the filtered data and its shape\n",
        "filtered_buoy_data_head = filtered_buoy_data.head()\n",
        "filtered_buoy_data_shape = filtered_buoy_data.shape\n",
        "\n",
        "filtered_buoy_data_head, filtered_buoy_data_shape\n",
        "\n"
      ],
      "metadata": {
        "id": "N1Y0ZMRn7Qo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alignment & Sequences"
      ],
      "metadata": {
        "id": "Axkq0Nb6-4ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequences"
      ],
      "metadata": {
        "id": "JGCK297U-08v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Sequences Function\n",
        "\n",
        "# Function to create overlapping sequences from combined image and mask data\n",
        "def create_sequences(data, sequence_length=6):\n",
        "    sequences = []\n",
        "    for start in range(len(data) - sequence_length + 1):\n",
        "        seq = data[start:start + sequence_length]\n",
        "        sequences.append(seq)\n",
        "    return np.array(sequences)\n",
        "\n",
        "# Assuming 'images' is your array of SAR images\n",
        "sequence_length = 6  # This is your desired sequence length\n",
        "\n",
        "# Create sequences from the interpolated data\n",
        "sequences = create_sequences(images, sequence_length)\n",
        "\n",
        "print(f\"Created {len(sequences)} sequences of shape {sequences.shape}\")"
      ],
      "metadata": {
        "id": "udeyxbG77vDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alignment"
      ],
      "metadata": {
        "id": "XkJM6Wg9-wPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alignment Sequences\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "# Function to extract dates from filenames\n",
        "def extract_date_from_filename(filename):\n",
        "    match = re.search(r'\\d{8}', filename)\n",
        "    if match:\n",
        "        date_str = match.group()\n",
        "        try:\n",
        "            return datetime.strptime(date_str, '%Y%m%d')\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Error parsing date from {filename}: {e}\")\n",
        "    else:\n",
        "        raise ValueError(f\"No valid date found in filename: {filename}\")\n",
        "\n",
        "# Directory path where your SAR images are stored\n",
        "directory_path = '/content/drive/MyDrive/Processed_Images_10k_V1'\n",
        "\n",
        "# Automatically generate the list of filenames in the directory\n",
        "sar_image_filenames = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.tif')]\n",
        "\n",
        "# Extract dates from all filenames\n",
        "sequence_dates = [extract_date_from_filename(name) for name in sar_image_filenames]\n",
        "\n",
        "# Assuming you want to work with sequence end dates as before\n",
        "N = 6  # Length of your sequences\n",
        "sequence_end_dates = sequence_dates[N-1:]  # Include every date from the Nth image onwards as an end date\n",
        "\n",
        "# Convert sequence_end_dates to numpy.datetime64\n",
        "sequence_end_dates_np = np.array([np.datetime64(date) for date in sequence_end_dates])\n",
        "\n",
        "print(f\"Adjusted Sequence End Dates (Potential Sequences): {len(sequence_end_dates_np)}\")\n",
        "\n",
        "# Function to align data with sequence dates using a tolerance window\n",
        "def align_data_with_tolerance(sequence_dates, data, data_label, tolerance=pd.Timedelta('1 hour')):\n",
        "    aligned_data = []\n",
        "    for date in sequence_dates:\n",
        "        # Find data points within the tolerance window\n",
        "        mask = (data['time'] >= (date - tolerance)) & (data['time'] <= (date + tolerance))\n",
        "        closest_points = data.loc[mask]\n",
        "        if not closest_points.empty:\n",
        "            # Choose the closest timestamp within the window\n",
        "            closest_row = closest_points.iloc[(closest_points['time'] - date).abs().argmin()]\n",
        "            aligned_data.append(closest_row[data_label])\n",
        "        else:\n",
        "            aligned_data.append(np.nan)\n",
        "    return np.array(aligned_data)\n",
        "\n",
        "# Convert the 'time' column in merged_df to datetime format and make timezone naive\n",
        "merged_df['time'] = pd.to_datetime(merged_df['time'], utc=True).dt.tz_localize(None)\n",
        "\n",
        "# Align wave heights\n",
        "aligned_wave_heights = align_data_with_tolerance(sequence_end_dates_np, merged_df, 'WaveHeight')\n",
        "\n",
        "# Align wind speeds\n",
        "aligned_wind_speeds = align_data_with_tolerance(sequence_end_dates_np, merged_df, 'WindSpeed')\n",
        "\n",
        "# Ensure altimetry dataset time column is in datetime64 format and timezone naive\n",
        "normalized_altimetry_dataset['time'] = pd.to_datetime(normalized_altimetry_dataset['time'].values).astype('datetime64[ns]')\n",
        "if hasattr(normalized_altimetry_dataset['time'], 'tz'):\n",
        "    normalized_altimetry_dataset['time'] = normalized_altimetry_dataset['time'].dt.tz_localize(None)\n",
        "\n",
        "# Verify the conversion\n",
        "print(\"\\nAltimetry Dataset Time Samples (after conversion):\")\n",
        "print(normalized_altimetry_dataset['time'].values[:5])\n",
        "print(type(normalized_altimetry_dataset['time'].values[0]))\n",
        "\n",
        "# Align altimetry data with debugging\n",
        "def align_altimetry_data(sequence_dates, altimetry_data, tolerance=np.timedelta64(1, 'h')):\n",
        "    aligned_altimetry = []\n",
        "    for date in sequence_dates:\n",
        "        # Ensure date is numpy.datetime64\n",
        "        date_np = np.datetime64(date)\n",
        "\n",
        "        # Debugging: Print types and values\n",
        "        print(f\"Date: {date_np}, Type: {type(date_np)}\")\n",
        "        print(f\"Altimetry Time Values: {altimetry_data.time.values[:5]}, Type: {type(altimetry_data.time.values[0])}\")\n",
        "\n",
        "        time_diff = np.abs(altimetry_data.time.values - date_np)  # Ensure date_np is numpy.datetime64\n",
        "\n",
        "        # Debugging: Print the computed time_diff\n",
        "        print(f\"Time Differences: {time_diff[:5]}\")\n",
        "\n",
        "        if np.any(time_diff <= tolerance):\n",
        "            closest_time_index = np.argmin(time_diff)\n",
        "            vhm0_data = altimetry_data.isel(time=closest_time_index)['VHM0']\n",
        "            vhm0_feature = vhm0_data.mean(dim=['latitude', 'longitude']).values.item()\n",
        "            aligned_altimetry.append(vhm0_feature)\n",
        "        else:\n",
        "            aligned_altimetry.append(np.nan)\n",
        "    return np.array(aligned_altimetry)\n",
        "\n",
        "aligned_altimetry_features = align_altimetry_data(sequence_end_dates_np, normalized_altimetry_dataset)\n",
        "\n",
        "# Check for NaNs and lengths\n",
        "print(f\"Length of aligned_wave_heights: {len(aligned_wave_heights)}\")\n",
        "print(f\"Length of aligned_wind_speeds: {len(aligned_wind_speeds)}\")\n",
        "print(f\"Length of aligned_altimetry_features: {len(aligned_altimetry_features)}\")\n",
        "print(f\"Length of sequences: {len(sequences)}\")\n",
        "\n",
        "# Initialize a full valid indices array with False\n",
        "valid_indices_full = np.zeros(len(sequences), dtype=bool)\n",
        "valid_indices_full[:len(sequence_end_dates_np)] = ~np.isnan(aligned_wave_heights) & ~np.isnan(aligned_wind_speeds) & ~np.isnan(aligned_altimetry_features)\n",
        "\n",
        "# Debugging: Print valid_indices_full length and sample\n",
        "print(f\"Length of valid_indices_full: {len(valid_indices_full)}\")\n",
        "print(f\"Sample of valid_indices_full: {valid_indices_full[:10]}\")\n",
        "\n",
        "filtered_sequences = sequences[valid_indices_full]\n",
        "filtered_wave_heights = aligned_wave_heights[valid_indices_full[:len(aligned_wave_heights)]]\n",
        "filtered_wind_speeds = aligned_wind_speeds[valid_indices_full[:len(aligned_wind_speeds)]]\n",
        "filtered_altimetry_features = aligned_altimetry_features[valid_indices_full[:len(aligned_altimetry_features)]]\n",
        "\n",
        "# Check shapes and alignments\n",
        "print(f\"Filtered SAR Sequences shape: {filtered_sequences.shape}\")\n",
        "print(f\"Filtered Altimetry Features shape: {filtered_altimetry_features.shape}\")\n",
        "print(f\"Filtered Wave Heights shape: {filtered_wave_heights.shape}\")\n",
        "print(f\"Filtered Wind Speeds shape: {filtered_wind_speeds.shape}\")\n"
      ],
      "metadata": {
        "id": "erLeJouA8KSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "RfxIjeVP-pF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "Fy5F4cJB-lWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture\n",
        "\n",
        "from tensorflow.keras.layers import concatenate, Input, ConvLSTM2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the RMSE metric\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "# Define input shapes for SAR, altimetry data, and wind speed\n",
        "sar_input_shape = (6, 128, 128, 1)  # Adjust based on your SAR data\n",
        "altimetry_input_shape = (1,)  # SWH is a single feature\n",
        "wind_speed_input_shape = (1,)  # Wind Speed is also a single feature\n",
        "\n",
        "# SAR image input branch\n",
        "sar_input = Input(shape=sar_input_shape, name='sar_input')\n",
        "convlstm1 = ConvLSTM2D(32, (3, 3), activation='relu', return_sequences=True)(sar_input)\n",
        "convlstm2 = ConvLSTM2D(64, (3, 3), activation='relu', return_sequences=False)(convlstm1)\n",
        "flattened_sar = Flatten()(convlstm2)\n",
        "\n",
        "# Altimetry input branch\n",
        "altimetry_input = Input(shape=altimetry_input_shape, name='altimetry_input')\n",
        "dense_altimetry = Dense(32, activation='relu')(altimetry_input)  # Reduced size for single feature\n",
        "\n",
        "# Wind speed input branch\n",
        "wind_speed_input = Input(shape=wind_speed_input_shape, name='wind_speed_input')\n",
        "dense_wind_speed = Dense(32, activation='relu')(wind_speed_input)  # Reduced size for single feature\n",
        "\n",
        "# Concatenate inputs from all branches\n",
        "combined = concatenate([flattened_sar, dense_altimetry, dense_wind_speed])\n",
        "\n",
        "# Fully connected layers after combining inputs\n",
        "dense1 = Dense(128, activation='relu')(combined)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation='linear')(dropout)  # Predicting a single value: wave height\n",
        "\n",
        "# Final model assembly\n",
        "model = Model(inputs=[sar_input, altimetry_input, wind_speed_input], outputs=output)\n",
        "\n",
        "# Compile the model with MAE and RMSE as the metrics\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', root_mean_squared_error])\n"
      ],
      "metadata": {
        "id": "m_3puEfe8SQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "Zjz9arXJ-gDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Split the dataset into train+validation sets and a test set\n",
        "(X_train_seq_filtered, X_test_seq_filtered,\n",
        " X_train_alt_filtered, X_test_alt_filtered,\n",
        " X_train_wind_filtered, X_test_wind_filtered,\n",
        " y_train_filtered, y_test_filtered) = train_test_split(\n",
        "    filtered_sequences, filtered_altimetry_features, filtered_wind_speeds, filtered_wave_heights,\n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the train+validation set into separate training and validation sets\n",
        "(X_train_seq_filtered, X_validate_seq_filtered,\n",
        " X_train_alt_filtered, X_validate_alt_filtered,\n",
        " X_train_wind_filtered, X_validate_wind_filtered,\n",
        " y_train_filtered, y_validate_filtered) = train_test_split(\n",
        "    X_train_seq_filtered, X_train_alt_filtered, X_train_wind_filtered, y_train_filtered,\n",
        "    test_size=0.25, random_state=42  # Splits the 80% of data into 60% training and 20% validation\n",
        ")\n",
        "\n",
        "# Train your model using the training set and validate using the validation set\n",
        "history_filtered = model.fit(\n",
        "    [np.array(X_train_seq_filtered), np.array(X_train_alt_filtered), np.array(X_train_wind_filtered)],\n",
        "    y_train_filtered,  # Inputs and targets for training\n",
        "    validation_data=(\n",
        "        [np.array(X_validate_seq_filtered), np.array(X_validate_alt_filtered), np.array(X_validate_wind_filtered)],\n",
        "        y_validate_filtered),  # Inputs and targets for validation\n",
        "    epochs=100,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Save the training history\n",
        "history_file = '/content/drive/MyDrive/Models/history_filtered_sixsequence.pkl'\n",
        "with open(history_file, 'wb') as f:\n",
        "    joblib.dump(history_filtered.history, f)\n",
        "\n",
        "# Save the model\n",
        "model_file = '/content/drive/MyDrive/Models/my_model_with_r2_sixsequence.h5'\n",
        "model.save(model_file)\n",
        "\n",
        "# After training, evaluate the model on the test set which is completely unseen\n",
        "test_metrics = model.evaluate(\n",
        "    [np.array(X_test_seq_filtered), np.array(X_test_alt_filtered), np.array(X_test_wind_filtered)],\n",
        "    y_test_filtered, verbose=0\n",
        ")\n",
        "\n",
        "# Extract the metrics\n",
        "test_loss, test_mae, test_rmse, test_r2 = test_metrics\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "print(f\"Test RMSE: {test_rmse}\")\n",
        "print(f\"Test R-squared: {test_r2}\")\n"
      ],
      "metadata": {
        "id": "VPV4-2zi85dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI"
      ],
      "metadata": {
        "id": "GcBBzbo5-c54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Occlusion Sensitivity"
      ],
      "metadata": {
        "id": "caKbfgxq-Qce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the model\n",
        "model_file = '/content/drive/MyDrive/Models/my_model_with_r2.h5'\n",
        "model = load_model(model_file, custom_objects={'root_mean_squared_error': root_mean_squared_error, 'r_squared': r_squared})\n",
        "\n",
        "# Load the data\n",
        "# Ensure you have your data loaded here, e.g., filtered_sequences, filtered_altimetry_features, filtered_wave_heights, filtered_wind_speeds\n",
        "\n",
        "def apply_occlusion(input_data, patch_size=10, stride=5):\n",
        "    input_shape = input_data[0].shape\n",
        "    occluded_data = []\n",
        "    for i in range(0, input_shape[1] - patch_size + 1, stride):\n",
        "        for j in range(0, input_shape[2] - patch_size + 1, stride):\n",
        "            occluded_sample = np.copy(input_data)\n",
        "            occluded_sample[0, i:i + patch_size, j:j + patch_size, :] = 0\n",
        "            occluded_data.append(occluded_sample)\n",
        "    return np.array(occluded_data)\n",
        "\n",
        "def compute_occlusion_impact(model, input_data, true_label, patch_size=10, stride=5):\n",
        "    base_prediction = model.predict(input_data)\n",
        "    occluded_data = apply_occlusion(input_data, patch_size, stride)\n",
        "\n",
        "    occlusion_impacts = []\n",
        "    for occluded_sample in occluded_data:\n",
        "        occluded_prediction = model.predict(occluded_sample)\n",
        "        impact = np.abs(base_prediction - occluded_prediction).mean()\n",
        "        occlusion_impacts.append(impact)\n",
        "\n",
        "    occlusion_impacts = np.array(occlusion_impacts)\n",
        "    occlusion_map = occlusion_impacts.reshape((input_data[0].shape[1] - patch_size) // stride + 1,\n",
        "                                              (input_data[0].shape[2] - patch_size) // stride + 1)\n",
        "    return occlusion_map, base_prediction\n",
        "\n",
        "# Example to visualize the occlusion map\n",
        "sample_index = 0  # Index of the sample to check\n",
        "sample_data = [np.array([X_test_seq_filtered[sample_index]]),\n",
        "               np.array([X_test_alt_filtered[sample_index]]),\n",
        "               np.array([X_test_wind_filtered[sample_index]])]\n",
        "true_label = y_test_filtered[sample_index]\n",
        "\n",
        "# Compute occlusion impact\n",
        "occlusion_map, base_prediction = compute_occlusion_impact(model, sample_data, true_label)\n",
        "\n",
        "# Normalize occlusion map for better visualization\n",
        "occlusion_map = occlusion_map / np.max(occlusion_map)\n",
        "\n",
        "# Visualize the occlusion impact for SAR data\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(occlusion_map, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.title(f'Occlusion Impact for SAR Data for Sample {sample_index + 1}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7c6S3c35-HkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrated Gradients"
      ],
      "metadata": {
        "id": "_MJ6JcQo-Vpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERGRATED GRADIENTS\n",
        "\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "\n",
        "# Define a score function\n",
        "score_function = CategoricalScore(0)\n",
        "\n",
        "# Initialize the saliency object\n",
        "saliency = Saliency(model, model_modifier=ReplaceToLinear(), clone=False)\n",
        "\n",
        "# Function to compute integrated gradients\n",
        "def compute_integrated_gradients(input_data):\n",
        "    return saliency(score_function, input_data, smooth_samples=20, smooth_noise=0.20)\n",
        "\n",
        "# Check integrated gradients for a few samples\n",
        "for i in range(3):  # Check for first 3 samples\n",
        "    sample = [np.array(X_test_seq_filtered[i:i+1]), np.array(X_test_alt_filtered[i:i+1]), np.array(X_test_wind_filtered[i:i+1])]\n",
        "    integrated_gradients = compute_integrated_gradients(sample)\n",
        "\n",
        "    # Visualize the integrated gradients for SAR data\n",
        "    for t in range(integrated_gradients[0].shape[1]):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(integrated_gradients[0][0, t, :, :], cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Integrated Gradients for SAR Data at Time Step {t+1}')\n",
        "        plt.show()\n",
        "\n",
        "    # Visualize the integrated gradients for Altimetry data\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(integrated_gradients[1][0])\n",
        "    plt.title('Integrated Gradients for Altimetry Data')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize the integrated gradients for Wind Speed data\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(integrated_gradients[2][0])\n",
        "    plt.title('Integrated Gradients for Wind Speed Data')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "oNTFsRYl-M5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}